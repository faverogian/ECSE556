{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ECSE 556 Homework 1\n",
    "Regression\n",
    "Gian Favero\n",
    "October 6th, 2023\n",
    "'''\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import feature and response data\n",
    "feature = np.genfromtxt('Data/gdsc_expr_postCB.csv', delimiter=',')\n",
    "\n",
    "response = np.genfromtxt('Data/gdsc_dr.csv', delimiter=',')\n",
    "response = response[7,:] # Select the response vector for the 7th drug, Doxurubicin\n",
    "\n",
    "# Remove all columns that have NaN values in the response vector\n",
    "feature = feature[:,~np.isnan(response)]\n",
    "response = response[~np.isnan(response)]\n",
    "\n",
    "# Transpose the feature matrix and response vector\n",
    "feature = feature[1:, :].T # Remove the first row (gene names) and transpose\n",
    "response = response.T\n",
    "\n",
    "# Scale the feature matrix\n",
    "scaler = StandardScaler()\n",
    "feature = scaler.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Define range of alpha values to test\n",
    "alpha = [0.01, 0.1, 0.3, 0.5, 0.9]\n",
    "num_features = []\n",
    "\n",
    "# Perform Lasso regression for each alpha value\n",
    "for val in alpha:\n",
    "    lasso = Lasso(alpha=val)\n",
    "    lasso.fit(feature, response)\n",
    "    weights = lasso.coef_\n",
    "    num_features.append(len(weights[weights != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of features vs. alpha\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(alpha, num_features)\n",
    "plt.plot(alpha, num_features)\n",
    "plt.xlabel('Alpha')\n",
    "plt.xticks(alpha)\n",
    "plt.ylabel('Number of Features')\n",
    "plt.title('Number of Features vs. Alpha')\n",
    "plt.show()\n",
    "\n",
    "# Make accompanying table\n",
    "import prettytable as pt\n",
    "table = pt.PrettyTable(['Alpha', 'Number of Features'])\n",
    "for i in range(len(alpha)):\n",
    "    table.add_row([alpha[i], num_features[i]])\n",
    "table.title = 'Number of Features vs. Alpha'\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Define inner and outer loop cross validation folds\n",
    "inner_loop = KFold(n_splits=3)\n",
    "outer_loop = KFold(n_splits=4)\n",
    "\n",
    "# Define range of alpha values to test\n",
    "param_grid = {'alpha': [0.01, 0.1, 0.3, 0.5, 0.9]}\n",
    "\n",
    "# Initialize lists to store the performance metrics\n",
    "MSEs = []\n",
    "spearman_correlation = []\n",
    "p_values = []\n",
    "alpha = []\n",
    "results = []\n",
    "\n",
    "for train_idx, test_idx in outer_loop.split(feature):\n",
    "    # Split the outer loop fold into training and testing sets\n",
    "    x_train = feature[train_idx]\n",
    "    x_test = feature[test_idx]\n",
    "    y_train = response[train_idx]\n",
    "    y_test = response[test_idx]\n",
    "\n",
    "    # Perform the inner loop cross validation using a grid search\n",
    "    hyper_tuning = GridSearchCV(estimator=Lasso(), param_grid=param_grid, cv=inner_loop)\n",
    "\n",
    "    # Fit the tuned model on the training set\n",
    "    hyper_tuning.fit(x_train, y_train) \n",
    "    hyper, optimal_model, cv_result = hyper_tuning.best_params_, hyper_tuning.best_estimator_, hyper_tuning.cv_results_\n",
    "\n",
    "    # Evaluate the optimal model on the outer loop test set\n",
    "    response_pred = optimal_model.predict(x_test)\n",
    "    mse = np.mean((response_pred - y_test)**2)\n",
    "    spearman, p = spearmanr(response_pred, y_test)\n",
    "\n",
    "    # Store the performance metrics\n",
    "    alpha.append(hyper['alpha'])\n",
    "    results.append(cv_result)\n",
    "    MSEs.append(mse)\n",
    "    spearman_correlation.append(spearman)\n",
    "    p_values.append(p)\n",
    "\n",
    "# Make accompanying tables\n",
    "import prettytable as pt\n",
    "\n",
    "Fold_table = pt.PrettyTable(['Alpha', 'Mean Squared Error', 'Spearman Correlation', 'p-value'])\n",
    "\n",
    "for i in range(len(MSEs)):\n",
    "    Fold_table.add_row([alpha[i], MSEs[i].round(3), spearman_correlation[i].round(3), p_values[i].round(7)])\n",
    "\n",
    "Fold_table.title = 'Model Performance Across Each Outer Loop Fold'\n",
    "\n",
    "print(Fold_table)\n",
    "\n",
    "Average_table = pt.PrettyTable(['Mean Squared Error', 'Spearman Correlation'])\n",
    "Average_table.add_row([np.mean(MSEs).round(3), np.mean(spearman_correlation).round(3)])\n",
    "Average_table.title = 'Average Model Performance'\n",
    "\n",
    "print(Average_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Mean of response vector\n",
    "baseline_mean = DummyRegressor(strategy='mean')\n",
    "\n",
    "# Median of response vector\n",
    "baseline_median = DummyRegressor(strategy='median')\n",
    "\n",
    "# Find MSE for each baseline model using cross validation\n",
    "MSE_mean = cross_val_score(baseline_mean, feature, response, scoring='neg_mean_squared_error', cv=4)\n",
    "MSE_median = cross_val_score(baseline_median, feature, response, scoring='neg_mean_squared_error', cv=4)\n",
    "\n",
    "# Make accompanying table\n",
    "import prettytable as pt\n",
    "\n",
    "baseline_table = pt.PrettyTable(['Model', 'Mean Squared Error'])\n",
    "baseline_table.add_row(['Mean', -1*np.mean(MSE_mean).round(3)])\n",
    "baseline_table.add_row(['Median', -1*np.mean(MSE_median).round(3)])\n",
    "baseline_table.title = 'Baseline Model Performances'\n",
    "print(baseline_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
