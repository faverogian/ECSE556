\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{parskip}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[table]{xcolor}
\usepackage{float}
\usepackage[framed, numbered]{mcode}
\usepackage[margin=3cm]{geometry}
\usepackage{subfig}

\usepackage{biblatex} %Imports biblatex package
\addbibresource{references.bib} %Import the bibliography file

\hypersetup{
        hidelinks
}

\lstset{basicstyle=\fontsize{9}{12}\ttfamily}

\begin{document}

\title{Machine Learning in Network Biology\\Homework 1}
\author{
        Dr. Amin Emad \\ \\
        Gian Favero: 261157396 \\
}
\date{October 6th, 2023}
\maketitle
\newpage

\tableofcontents
\newpage

\section{Introduction}
Transcriptomic data refers to a collection of RNA molecules, or transcripts, present in cell tissues at a reference time. Typically, these molecules are generated in a cell through the process of transcription of DNA into RNA. This data contains insights into the expression levels of genes in a cell which can be used to predict various items like disease, stimuli response, and development, among others.

Analyzing transcriptomic data is a complex task due to the large number of genes present in a cell. However, successful analysis of this data can lead to important discoveries in gene function, networks, and pathways involved in biological processes. This is especially important in the field of network biology where the goal is to understand the structure and function of biological networks.

Several classical machine learning approaches can be used when analyzing transcriptomic data. These include dimensionality reduction, clustering, and regression. All three of these particular approaches will be explored in this assignment using popular Python libraries to gain some insight into the data provided. Gene expression data was provided alongside this assignment(\verb|gdsc_expr_postCB.csv|) and drug response data (\verb|gdsc_dr.csv|). The file contains rows that begin with a gene name and are taken to be features of the data, while columns represent gene lines, or instances. 

\section{Part 1: Dimensionality Reduction}
Dimensionality reduction is a technique used to reduce the number of features in a dataset. This is done by projecting the data onto a lower dimensional space and finding a new, simpler set of features that can be used to represent the original data. Memory usage, computational efficiency, and visualization are all advantages of dimensionality reduction.

Several dimensionality reduction techniques are explored in this assignment. These include Principal Component Analysis (PCA), UMAP, and t-SNE algorithms.

\subsection{Principal Component Analysis}
\subsubsection{Theory}

The following is adapted from Goodfellow et al \cite{GoodBengCour16}.

PCA is a lossy compression technique that is used to reduce the dimensionality of a dataset. It is lossy because the original data is stored in a way that uses less memory, but loses some precision in the process. Ideally, the data is compressed in a way that preserves the most important information.

For a set of \textit{m} data points $\textbf{x}^{(i)}$ $\in$ $\mathbb{R}^n$, PCA finds a lower dimensional representation of the data by representing it as a code vector $\textbf{c}^{(i)}$ $\in$ $\mathbb{R}^l$ where $l < n$. The goal of PCA is to find a function $f$ such that $\textbf{c}^{(i)}$ = $f(\textbf{x}^{(i)})$ and a decoding function $g$ which provides $x \approx g(f(x))$. This is simply done using a decoding matrix, $\textbf{D} \in \mathbb{R}^{n \times l}$ which has columns that are orthogonal and have a unit norm. Following this, the code vector is given by:
\begin{align*}
        f(\textbf{x}) = \textbf{c} = \textbf{D}^T\textbf{x}
\end{align*}
and the reconstructed input is given by:
\begin{align*}
        g(\textbf{c}) = \textbf{x} = \textbf{D}\textbf{D}^T\textbf{x}
\end{align*} 

Using a closed-form or numerical optimization method, the decoding matrix can be found by minimizing the reconstruction error, $||\textbf{x} - \textbf{D}\textbf{D}^T\textbf{x}||_2^2$ across each point in the dataset.
 
Often times $l$ is a parameter selected by the user. For visual purposes, $l$ is typically chosen as 2 such that the compressed data can be plotted on a Cartesian plane.  

\subsubsection{Implementation}
A PCA algorithm was implemented using the \verb|sklearn.decomposition.PCA| library in addition to peripheral libraries for data manipulation (\verb|numpy|) and visualization (\verb|matplotlib|).

\subsubsection{Results}

\subsection{UMAP}
\subsubsection{Theory}

\subsubsection{Implementation}

\subsubsection{Results}

\subsection{t-SNE}
\subsubsection{Theory}

\subsubsection{Implementation}

\subsubsection{Results}

\subsection{Comparison of Dimensionality Reduction Techniques}

\section{Part 2: Clustering}
\subsection{Agglomerative Clustering}
\subsubsection{Theory}

\subsubsection{Implementations}

\subsubsection{Results}

\subsection{K-Means Clustering}
\subsubsection{Theory}

\subsubsection{Implementation}

\subsubsection{Results}

\subsection{Comparison of Clustering Techniques}
\subsubsection{Jaccard Similarity}

\subsubsection{Rand Index}

\subsubsection{Adjusted Rand Index}

\newpage
\printbibliography

\end{document}
