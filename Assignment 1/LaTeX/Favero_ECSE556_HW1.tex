\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{parskip}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[table]{xcolor}
\usepackage{float}
\usepackage[framed, numbered]{mcode}
\usepackage[margin=3cm]{geometry}
\usepackage{subfig}
\usepackage[numbers]{natbib}
\usepackage{diagbox}

\hypersetup{
        hidelinks
}

\lstset{basicstyle=\fontsize{9}{12}\ttfamily}

\begin{document}

\title{Machine Learning in Network Biology\\Homework 1}
\author{
    Dr. Amin Emad \\ \\
    Gian Favero: 261157396 \\
}
\date{October 6th, 2023}
\maketitle
\newpage

\tableofcontents
\newpage

\section{Introduction}
Transcriptomic data refers to a collection of RNA molecules, or transcripts, present in cell tissues at a reference time. Typically, these molecules are generated in a cell through the process of transcription of DNA into RNA. This data contains insights into the expression levels of genes in a cell which can be used to predict various items like disease, stimuli response, and development, among others \cite{noauthor_transcriptome_nodate}.

Several classical machine learning approaches can be used when analyzing transcriptomic data. These include dimensionality reduction, clustering, and regression. All three of these particular approaches will be explored in this assignment using popular Python libraries to gain some insight into the data provided. Gene expression data was provided alongside this assignment(\verb|gdsc_expr_postCB.csv|) and drug response data (\verb|gdsc_dr.csv|). The file contains rows that begin with a gene name and are taken to be features of the data, while columns represent gene lines, or instances. 

\section{Part 1: Dimensionality Reduction}
Dimensionality reduction is a technique used to reduce the number of features in a dataset. This is done by projecting the data onto a lower dimensional space and finding a new, simpler set of features that can be used to represent the original data. Memory usage, computational efficiency, and visualization are all advantages of dimensionality reduction.

Several dimensionality reduction techniques are explored in this assignment. These include Principal Component Analysis (PCA), UMAP, and t-SNE algorithms.

\subsection{Theory}
\subsubsection{Principal Component Analysis}
The following is adapted from Goodfellow et al. \cite{GoodBengCour16}.

For a set of \textit{m} data points $\textbf{x}^{(i)}$ $\in$ $\mathbb{R}^n$, PCA finds a lower dimensional representation of the data by representing it as a code vector $\textbf{c}^{(i)}$ $\in$ $\mathbb{R}^l$ where $l < n$. The goal of PCA is to find a function $f$ such that $\textbf{c}^{(i)}$ = $f(\textbf{x}^{(i)})$ and a decoding function $g$ which provides $\textbf{x} \approx g(f(\textbf{x}))$. This is simply done using a decoding matrix, $\textbf{D} \in \mathbb{R}^{n \times l}$ which has columns that are orthogonal and have a unit norm. Following this, the code vector is given by:
\begin{align*}
    \textbf{c} = f(\textbf{x}) = \textbf{D}^T\textbf{x}
\end{align*}
and the reconstructed input is given by:
\begin{align*}
    \textbf{x} \approx g(\textbf{c}) = \textbf{D}\textbf{D}^T\textbf{x}
\end{align*} 

Using a closed-form or numerical optimization method, the decoding matrix can be found by minimizing the reconstruction error, $||\textbf{x} - \textbf{D}\textbf{D}^T\textbf{x}||_2^2$ across each point in the dataset.
 
$l$ is typically the only parameter selected by the user. For visual purposes, $l$ is typically chosen as 2 such that the compressed data can be plotted on a Cartesian plane.

\subsubsection{t-SNE}
t-SNE was proposed by Van der Maaten et al. \cite{vanDerMaaten2008} as a technique for visualizing high-dimensional data. At a high-level, the algorithm maps each data point to a location on a 2D or 3D map that can reveal the structure at many scales. The resulting map effectively reveals low-dimensional manifolds on which the data points lie.

t-SNE algorithms are incorporated into the \verb|scikit-learn| Python library. The algorithm is capable of optimizing a dimensionality reduction on a dataset based on a number of parameters selected by a user. Some of these include the number of dimensions and the perplexity, which dictate the manner in which the algorithm structures the lower-dimension manifolds in which the data lies.

\subsubsection{UMAP}
UMAP is a novel non-linear manifold learning technique by McInnes et al. \cite{mcinnes_umap_2020}. UMAP is a scalable algorithm that is comparable to t-SNE in visualization, but superior in runtime and preservation of global structure.

UMAP algorithms have been abstracted into a Python library, \verb|umap-learn|, which is capable of optimizing a dimensionality reduction on a dataset based on a number of parameters selected by a user. Some of these include the number of dimensions, the number of neighbors and the minimum distance between points, which play roles in the generalization of the algorithm, the trends identified (local or global), and the distinction between clusters.

\subsection{Implementation}
Brief data pre-processing was done before implementing the different dimensionality reduction techniques. Commonly used libraries such as \verb|numpy| and \verb|matplotlib| were imported, before the data was loaded into a numpy array, stripped of its first row (instance headers) and its first column (feature headers) and transposed such that the data was in the form of a $n \times m$ matrix where $n$ is the number of instances and $m$ is the number of features.

The dimensionality reduction techniques were then implemented. The \verb|scikit-learn| library was used to implement PCA and t-SNE, while the \verb|umap-learn| library was used to implement UMAP. Specifically, the \verb|sklearn.decomposition.PCA|, \verb|umap|, and \verb|sklearn.manifold| classes were used for \href{https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html}{PCA}, \href{https://umap-learn.readthedocs.io/en/latest/parameters.html}{UMAP}, and \href{https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html}{t-SNE} respectively. Please refer to the linked documentation for more information on the class parameters and their default values.

First, the techniques were implemented with their default parameters with the exception of the number of dimensions, which was set to 2 for all techniques. The results of this implementation are shown in Figure \ref{fig:default_dimred}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/default_dimred.png}
    \caption{PCA (left), UMAP (middle), and t-SNE (right) reduction using default parameters.}
    \label{fig:default_dimred}
\end{figure}

\vspace{-0.5cm}

A quick implementation with the default parameters of each technique gives some insight into the nature of the performance. As seen in Figure \ref{fig:default_dimred}, distinct regions of point clusters have been generated in both the UMAP and t-SNE plots. The PCA plot, however, does not show any distinct clusters and instead shows a large cloud of points with no discernible structure. Knowing that both UMAP and t-SNE are non-linear techniques, it can be deduced that perhaps linear methods, such as PCA, are not the most suitable dimensionality reduction approaches for this dataset.

In addition to the visual inspection of the plots, the runtime of each technique was also recorded. The runtime of each technique is shown in Table \ref{tab:default_dimred}.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Method} & \textbf{Runtime (s)} \\
        \hline
        PCA & 0.383  \\
        \hline
        UMAP & 5.850 \\
        \hline
        t-SNE & 3.916 \\
        \hline
    \end{tabular}
    \caption{Runtime of each technique using default parameters.}
    \label{tab:default_dimred}
\end{table}

\vspace{-0.5cm}

A point to note is that while both UMAP and t-SNE have generated distinct clusters, the clusters are not identical in shape or density. UMAP has produced denser clusters that are closer to the origin, while t-SNE has produced a similar amount of clusters that are more spread out, both in terms of point-to-point and cluster-to-cluster. To examine the non-linear techniques further, parameters of interest were changed in isolation to analyze their effect on the resulting plots. 

\subsubsection{UMAP Parameter Analysis}
A sequence of reductions with varying number of neighbors in UMAP is observed in Figure \ref{fig:UMAP_vary}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/UMAP_vary.png}
    \caption{UMAP reduction of the data varying the number of neighbors.}
    \label{fig:UMAP_vary}
\end{figure}

\vspace{-0.5cm}

Visually, adjusting the number of neighbors considered in a UMAP reduction provides a different insight into the relationships between data points. As seen in Figure \ref{fig:UMAP_vary}, the number of clusters identified by the algorithm decreases as the number of neighbors increases. It can be discerned then, that considering more neighbors captures broader trends in the data, while considering fewer neighbors captures more local trends.

A sequence of reductions with varying minimum distance in UMAP is observed in Figure \ref{fig:UMAP_vary2}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/UMAP_vary2.png}
    \caption{UMAP reduction of the data varying the minimum distance.}
    \label{fig:UMAP_vary2}
\end{figure}

\vspace{-0.5cm}

From inspection, the minimum distance parameter seems to have a spread effect on the clusters and data points. As the minimum distance increased, the clusters reduced in number and became more spread out. This is likely due to the fact that the minimum distance parameter is used to control the effective minimum distance between points. As the minimum distance increases, the algorithm is forced to spread out the clusters to meet the minimum distance requirement.

Depending on the objective of the dimensionality reduction, both the number of neighbors and the minimum distance parameters can be used to control the generalization of the algorithm. If the goal is to identify broad trends in the data, then a higher number of neighbors and a lower minimum distance should be used. If the goal is to identify local trends in the data, then a lower number of neighbors and a higher minimum distance should be used.

\subsubsection{t-SNE Parameter Analysis}
A sequence of reductions with varying perplexity in t-SNE is observed in Figure \ref{fig:TSNE_vary}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/t-sne_vary.png}
    \caption{t-SNE reduction of the data varying the perplexity.}
    \label{fig:TSNE_vary}
\end{figure}

\vspace{-0.5cm}

Similar to the number of neighbors parameter in UMAP, the perplexity parameter in t-SNE controls the number of neighbors considered when reducing the dimensionality of the data. As seen in Figure \ref{fig:TSNE_vary}, the number of clusters identified by the algorithm decreases as the perplexity increases. It can be similarly discerned then that considering more neighbors captures broader trends in the data, while considering fewer neighbors captures more local trends.

Again, depending on the objective of the dimensionality reduction, the perplexity parameter can be used to control the generalization of the algorithm. If the goal is to identify broad trends in the data, then a higher perplexity should be used. If the goal is to identify local trends in the data, then a lower perplexity should be used.

\subsection{Discussion of Results}
\begin{enumerate}
    \item \textbf{Do you see different clusters forming (visually)? How much correspondence exist between clusters that you find in different methods?}

    Different clusters formed visually in the UMAP and t-SNE plots. The clusters in the UMAP plot were denser and closer to the origin, while the clusters in the t-SNE plot were more spread out. The clusters in the PCA plot were not visually discernible.  

    Varying the parameters of both the UMAP and t-SNE algorithms resulted in different clusters being formed. The number of clusters identified by the algorithms decreased as the number of neighbors or perplexity increased. This is due to the fact that considering more neighbors or a higher perplexity captures broader trends in the data, while considering fewer neighbors or a lower perplexity captures more local trends. Therefore, the non-linear techniques are surely capable of producing high correspondence, but the parameters must be tuned to achieve this correspondence. However, neither UMAP nor t-SNE had correspondence with the PCA plot.

    \item \textbf{What is the difference between the three methods above? Use pros/cons to compare them against each other.}
    
    PCA is a linear dimensionality reduction technique that is capable of reducing the dimensionality of a dataset while preserving the maximum amount of variance. PCA is a fast algorithm (was the fastest of the three), however, PCA is not capable of capturing non-linear relationships in the data and is therefore not suitable for datasets with non-linear relationships. In addition, PCA relies on lossy compression, meaning that the original data cannot be fully recovered from the compressed data.

    UMAP is a non-linear, highly customizable, dimensionality reduction technique that is capable of reducing the dimensionality of a dataset while preserving the global structure of the data. In this implementation, UMAP was the slowest of the three algorithms as a result of its high degree of complexity. However, UMAP was best suited for this dataset as it was able to capture the non-linear relationships in the data and produce dense clusters from the data capturing both local and global trends.

    t-SNE is another non-linear dimensionality reduction technique that was capable of forming clusters with this data. t-SNE was faster than UMAP, but could not generate clusters as dense as UMAP on a 2D plot. However, like UMAP, t-SNE is tunable and can be used to capture local trends in the  highly effectively.
\end{enumerate}

\section{Part 2: Clustering}
\subsection{Theory}
\subsubsection{Agglomerative Clustering}
Agglomerative clustering is a clustering approach that works by first partitioning a dataset into singleton clusters, and then iteratively merging the clusters together until there is only one cluster remaining \cite{mullner_modern_2011}. There are several parameters that govern the algorithm: linkage, distance metric, and, obviously, the number of clusters. Linkage refers to the method used to calculate the distance between clusters, while the distance metric refers to the method used to calculate the distance between points. 

\subsubsection{K-Means Clustering}
K-means clustering is a traditional unsupervised algorithm used to partition a dataset into $k$ clusters. The algorithm works by first initializing $k$ centroids, and then iteratively assigning each point to the cluster with the closest centroid, and then updating the centroids based on the new cluster assignments until a steady state is reached. The algorithm is sensitive to the initial centroid positions, and therefore, the algorithm is typically run multiple times with different initialization to ensure that the global minimum is reached.

\subsection{Implementation}
The \verb|sk.learn.cluster.AgglomerativeClustering| function was used to implement agglomerative clustering. Initially, the function was used with its default parameters with the exception of the number of clusters, which was set to 3. 

The \verb|sklearn.cluster.KMeans| function was used to implement K-means clustering with a specification of 3 clusters. Given that the initial clusters are assigned randomly, the following results are based on an individual run of the algorithm and are therefore are not fully reproducible without setting a random seed.

\subsubsection{Jaccard Similarity with Default Parameters}
The computed clusters of each algorithm were compared using the Jaccard similarity metric implemented via the \verb|sklearn.metrics.jaccard_score| function. An iterative approach was conducted to compute the similarity of the nine permutations of the three algorithms. The results of this approach are shown in Table \ref{tab:jaccard_default}. Note that the rows represent the clusters computed by the agglomerative algorithm, while the columns represent the clusters computed by the K-means clustering algorithm.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline 
        \diagbox{Agg.}{K-means} & \textbf{Cluster 1} & \textbf{Cluster 2} & \textbf{Cluster 3} \\
        \hline
        \textbf{Cluster 1} & 0.892 & 0.000 & 0.044 \\
        \hline
        \textbf{Cluster 2} & 0.000 & 1.000 & 0.000 \\
        \hline
        \textbf{Cluster 3} & 0.022 & 0.000 & 0.864 \\
        \hline
    \end{tabular}
    \caption{Jaccard similarity of the clusters using default parameters.}
    \label{tab:jaccard_default}
\end{table}

\vspace{-0.5cm}

The Jaccard similarity metric is a measure of the similarity between two sets. The metric is defined as the size of the intersection of the sets divided by the size of the union of the sets. In this case, the sets are the clusters identified by the algorithms. The Jaccard similarity metric is a value between 0 and 1, where 0 indicates no similarity and 1 indicates perfect similarity.

From Table \ref{tab:jaccard_default}, it can be seen that the clusters identified by the algorithms are not identical, however there is a high degree of similarity. Note that the values were rounded to 3 decimal places for readability. 

\subsubsection{Rand Index with Default Parameters}
The computed clusters of each algorithm were compared using the Rand index metric implemented via the \verb|sklearn.metrics.adjusted_rand_score| function. Adjusting for 3 decimal places, the Rand-index of the two algorithms at default parameters was approximately \textbf{0.918}.

\subsubsection{Adjusted Rand Index with Default Parameters}
The computed clusters of each algorithm were compared using the adjusted Rand index metric implemented via the \verb|sklearn.metrics.adjusted_rand_score| function. Adjusting for 3 decimal places, the adjusted Rand-index of the two algorithms at default parameters was approximately \textbf{0.825}.

\subsection{Analysis of Agglomerative Clustering}
Agglomerative clustering was implemented separately with average linkage and Euclidean and cosine distance metrics. A Jaccard similarity analysis of this implementation are shown in Table \ref{tab:average_agglo}.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline 
        \diagbox{Euclidean}{Cosine} & \textbf{Cluster 1} & \textbf{Cluster 2} & \textbf{Cluster 3} \\
        \hline
        \textbf{Cluster 1} & 1.000 & 0.000 & 0.000 \\
        \hline
        \textbf{Cluster 2} & 0.000 & 1.000 & 0.000 \\
        \hline
        \textbf{Cluster 3} & 0.000 & 0.000 & 1.000 \\
        \hline
    \end{tabular}
    \caption{Jaccard similarity of the clusters using average linkage and Euclidean and cosine distance.}
    \label{tab:average_agglo}
\end{table}

\vspace{-0.5cm}

Interestingly, the clusters produced by the average linkage algorithm are identical regardless of the distance metric used. The clusters found via Euclidean distance are identical to the clusters found via cosine distance. A hypothesis for this is that the average linkage algorithm inferred on this dataset is not sensitive to the distance metric used. Correspondingly, the Rand-Index and adjusted Rand-Index were found to be \textbf{1.0}. Therefore, it can be concluded that the clusters produced by the average linkage algorithm are identical regardless of the distance metric used.


A further analysis was conducted to examine the similarity between the clusters produced by the average linkage algorithm and the K-means algorithm. The results of this analysis are shown in Table \ref{tab:agglo_euc}.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline 
        \diagbox{Agg.}{K-means} & \textbf{Cluster 1} & \textbf{Cluster 2} & \textbf{Cluster 3} \\
        \hline
        \textbf{Cluster 1} & 0.475 & 0.002 & 0.000 \\
        \hline
        \textbf{Cluster 2} & 0.174 & 0.000 & 0.000 \\
        \hline
        \textbf{Cluster 3} & 0.349 & 0.000 & 0.017 \\
        \hline
    \end{tabular}
    \caption{Jaccard similarity of the clusters using average linkage and K-means.}
    \label{tab:agglo_euc}
\end{table}

\vspace{-0.5cm}

In addition to the Jaccard similarity metric, the Rand index and adjusted Rand index were also computed. The Rand index was approximately \textbf{0.382}, while the adjusted Rand index was approximately \textbf{0.0}.

Average linkage clustering with Euclidean and cosine distance metrics produced clusters that were not similar to the clusters produced by the K-means algorithm. This is likely due to the fact that the average linkage algorithm is sensitive to outliers, and therefore, the clusters produced by the algorithm are not as dense as the clusters produced by the K-means algorithm. 

\section{Regression}

\newpage

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
